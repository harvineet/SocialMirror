<html>
<body>

<h3>Visualisation of user vectors</h3>
2-dimensional visualisation of user vectors (2000 randomly selected users) using <a href="http://lvdmaaten.github.io/tsne/">t-SNE</a></br>
Colour represents geography of the user.</br>
<img src="plots/embed_users_random_2000.png" width="800" height="800" />
</br><a href="plots/nearest_users_scatterplot1.png">Link1</a>,<a href="plots/nearest_users_scatterplot2.png">Link2</a>
<h4> 17 Mar </h4>

<h3>Extracting contexts for learning node representations</h3>

Procedure for obtaining contexts for user tweets:</br>
</br>
1. Starting from hashtag traces, i.e. time-ordered set of tweets on the hashtags. For each hashtag,</br>
</br>
   1.1 create hashtag graph (directed), with an edge from user tweet t_1 to t_2 if timestamp of t_1 and t_2 differ by atmost 12 hours and they're from the same geography.</br>
</br>
   1.2 For each tweet (node in the hashtag graph), extract path starting from the corresponding node and traversing the DAG to the left (in reverse direction of edges) and right, selecting from neighbouring nodes at random, i.e., doing a random walk of length m (context width parameter) in each direction. The max length of path thus obtained is 2*m+1 (m is set to 5). One such path is sampled for each tweet in the hashtag graph (this is a parameter, d, set to 1).</br>
</br>

Training parameters for word2vec:</br>
</br>
Hierarchical softmax, Skip-gram model with context length: 10 (i.e. maximum distance between current and predicted word is 10), </br>
Vector dimension: 100, sub-sampling threshold: 10^-4 (users with frequency more than this threshold are down-sampled), </br>
Minimum count: 200 (i.e. users occurring less than 200 times in corpus are removed, done to reduce vocabulary size)</br>
</br>

Time taken for creating sentence file: approx. 6 hours (using 3 parallel processes, can be increased), most of the time is taken in creating adjacency list for hashtag graphs and the bottleneck being hashtags with a large number of closely-timed tweets, such as 'ff', 'rt', 'win'.</br>
Dataset characteristics:</br>
Number of hashtags: 3617312</br>
Number of unique users: 2628833</br>
Number of sentences (or paths): 175881363. The plot for the frequency distribution of path lengths (paths with nodes less than 2, i.e. isolated nodes, were removed):</br>
<img src="plots/FrequencyPlotContextLength.png" width="400" height="400" /></br>

Considering all the sentences as a text corpus for input to word2vec,</br>
Size of corpus: 1525212745</br>
Size of vocabulary: 2628833</br>
The plot for cumulative distribution of user occurrences in the corpus is:</br>
<img src="plots/CumulativeFrequencyUserOccurrence.png" width="600" height="400" /></br>
So to reduce the vocabulary size to about 1 million, users occurring less than 200 times in the corpus have to be removed.

<h4> 04 Mar </h4>

<h3>Checking sparsity of contexts for learning node representations</h3>
Filtering users: Taking the median values for number of followees and number of tweets, filtered users such that, number of followees > 200 and number of tweets > 15. This resulted in a set of 1,001,525 users (so the vocabulary size for node vector training will be 1M).</br>
For this set of users, considering two ways of generating context:</br>
number of users with reciprocal links, i.e. for each user how many users have follower-following links to it, and</br>
number of tweets on same hashtags, i.e. number of tweets on the hashtags used by a user</br>
The distribution plots are (log scale in x-axis):</br>
<img src="plots/reciprocal_links.png" width="600" height="400" />
<img src="plots/same_tweets.png" width="600" height="400" />
 
<h4> 16 Feb </h4>

<a href="sentiment classification of tweets.pdf">Link</a> to ppt on approach taken in Sentiment140 for sentiment classification of tweets. <br>
<br>
Total number of tweets on hashtags which were mis-classified by CGNP:<br>
<img src="plots/prob_vs_numoftweets.png" width="600" height="400" />

<h3>Visualisation of hashtag vectors</h3>
Visualisation of hashtag histogram-of-count vectors using <a href="http://lvdmaaten.github.io/tsne/">t-SNE</a> and tf-idf measure in vectors for word relevance (top 150 hashtags by number of tweets are plotted). Reference <a href="https://github.com/turian/textSNE">textSNE</a><br>
<img src="plots/test-output.rendered_mostfreq.png" width="1200" height="800" />

<h4> 10 Feb </h4>

<h3>Counting flips in aggregated sentiment of consecutive tweets</h3>
Groups of 50 consecutive tweets (also tried 10,25,100) on the topic are considered and overall sentiment is calculated using majority vote (assigning group of consecutive tweets the sentiment with the most number of tweets). Number of flips using this aggregated sentiment is added as a feature for classification.<br>
AUC: 0.297<br>
Also, giving more weight to flips between "positive" and "negative" sentiments didn't give significant improvement. On an average, non-viral hashtags have higher number of flips than viral hashtags. The pdf plot is:<br>
<img src="plots/sentiment_flips.png" width="600" height="400" />

<h4> 18 Jan </h4>

<h3>Changes in sentiment of tweets on a particular topic</h3>
The x-axis represents the number of tweets on the topic over time. Here, tweets with different sentiment labels are separated out vertically.<br>
<a href="scatterplot.html">This link</a> contains the same plots but with jitter added in y-axis to separate out the closely-spaced points.<br>
-1: Negative (Red), 0: Neutral (Blue), 1: Positive (Green)
<h4> Non-viral topics </h4>
<img src="plots/sentiment_timelines/plots/nv.png" width="1100" height="250" />
<img src="plots/sentiment_timelines/plots/2nv.png" width="1100" height="250" />
<img src="plots/sentiment_timelines/plots/3nv.png" width="1100" height="250" />
<img src="plots/sentiment_timelines/plots/4nv.png" width="1100" height="250" />
<img src="plots/sentiment_timelines/plots/5nv.png" width="1100" height="250" />
<img src="plots/sentiment_timelines/plots/6nv.png" width="1100" height="250" />
<h4> Viral topics </h4>
<img src="plots/sentiment_timelines/plots/v.png" width="1100" height="250" />
<img src="plots/sentiment_timelines/plots/2v.png" width="1100" height="250" />
<img src="plots/sentiment_timelines/plots/3v.png" width="1100" height="250" />
<img src="plots/sentiment_timelines/plots/4v.png" width="1100" height="250" />
<img src="plots/sentiment_timelines/plots/5v.png" width="1100" height="250" />
<img src="plots/sentiment_timelines/plots/6v.png" width="1100" height="250" />

<h4> 4 Jan </h4>

<h3>Topic coherence measure using entropy of word distribution into different clusters</h3>
<img src="plots/word_spread_thresh.png" width="800" height="500" />

<h3>Classification accuracy under various settings using word vector representations</h3>
 
Using hashtag cluster (obtained using kmeans clustering on histogram of count vectors for each hashtag) as feature:
<pre>
Num of Word clusters	Num of Hashtag clusters		AUPRC
100			9				0.294
500			20				0.292
1000			10				0.296
1000			20				0.294
1000			30				0.292
1000			50				0.291
</pre>

Using histogram of count vectors obtained for each hashtag directly as features:</br>
<pre>
Num of Word clusters		AUPRC
500				0.258
1000				0.232
</pre>

Adding features obtained by applying PCA on the histogram of count vectors:</br>
<pre>
Principal Components used	AUPRC
PC1				0.285
PC1,PC2				0.295
PC1,PC2,PC3			0.289
</pre>

<h4> 17 Nov </h4>

<h3>Clustering hashtags using word vector representations</h3>

Dimension of word vector 200</br>
Vocabulary size 868581</br>
Number of word clusters 100</br>
<br>
Number of hashtag clusters 9</br>
Hashtag distribution among clusters: 400,105,55,192,178,522,762,392,204</br>

Sample clusters with likely category (not entirely):</br>
<a href="Segment1.txt">Cluster of hashtags (Italian)</a>
<a href="Segment2.txt">Cluster of hashtags (Latin)</a>
<a href="Segment4.txt">Cluster of hashtags (Sports)</a>
<a href="Segment8.txt">Cluster of hashtags (Government, Politics)</a>

<h4> 14 Oct </h4>

<h3>Word vector representations using GloVe and word2vec</h3>
Training corpus - November 2013 twitter crawl (35896465 tweets)</br>
Some basic pre-processing (removing user mentions, url, special characters, lowercase, etc.), tokenisation using whitespace only</br>
Training time: word2vec (222m22s), GloVe (150m19s)</br>
Some interesting observations from word vectors obtained using word2vec (200 dimension vectors):</br>
</br>
Query	- Similar words (cosine similarity)</br>
</br>
heartbleed - openssl, cve</br>
sfbatkid - batkid, rescue, makeawish</br>
bjp	- aap, Modi, Delhi</br>
snowinapril - whereisspring</br>
snowden - nsa</br>
</br>
Analogy task (A : B :: C : ?)</br>
</br>
king : queen :: man : (woman)</br>
mufc : football :: nba : (basketball)</br>
aap : kejriwal :: bjp : (modi)</br>

<br>Reference http://nlp.stanford.edu/projects/glove/, https://code.google.com/p/word2vec/
<h4> 7 Oct </h4>

<h3>Hashtag Segmentation adding Twitter based corpus in previous model</h3>
<pre>
(Hashtag)	(Previous Segmentation)	(Current Segmentation)

(Correct)
instagramnotworking	insta,gram,not,working	instagram,not,working
votekatniss	vote,kat,niss	vote,katniss
sonyxperia	sony,x,peri,a	sony,xperia
wweretweeting	w,were,tweet,ing	wwe,retweeting
beinghumanfinale	being,human,finale	beinghuman,finale
aprilfool	april,fool	aprilfool
bitcoinconf	bit,coin,conf	bitcoin,conf
tipsfornewdirectioners	tips,for,new,direction,ers	tips,for,new,directioners
bayernrealmadrid	bayern,real,madrid	bayern,realmadrid
happygoodfriday	happy,good,friday	happy,goodfriday
kxipvsrcb	kxipvsrcb	kxip,vs,rcb

(Incorrect)
ripzaynmalik	rip,zayn,malik	rip,zaynmalik
austinmahonetour	austin,mahone,tour	austinmahone,tour
britainsgottalent	britains,got,talent	britainsgottalent
coachellalive	coachella,live	coachellalive
wnbadraft	wnba,draft	w,nbadraft
happyeasterweekend	happy,easter,weekend	happyeaster,weekend
snowinapril	snow,in,april	snowin,april
kkrvsrcb	kk,rvs,rcb	kk,rvs,rcb
</pre>
<a href="tag_segments_tw.txt">Segmentation for hashtags using twitter corpus</a>
<br>Reference Twitter corpus from Rovereto Twitter N-Gram Corpus http://clic.cimec.unitn.it/amac/twitter_ngram/

<h4> 23 Sep </h4>

<h3>Hashtag Segmentation using Unigram and Bigram model</h3>
<pre>
bundyranch	bundy,ranch
nashto2mil	nash,to,2,mil
votetris	vote,tris
epnvsinternet	epn,vs,internet
scandalfinale	scandal,finale
mynypd	my,nypd
donaldsterling	donald,sterling
cancelcolbert	cancel,colbert
voicesave	voice,save
clivenbundy	clive,n,bundy
rhoareunion	rhoa,reunion
voteloki	vote,loki
bloodmoon	bloodmoon
kochbuysgop	koch,buys,gop
iamnotaliberalbecause	i,am,not,a,liberal,because
blm	blm
dialogomaduroporlapaz	dialogo,maduro,por,la,paz
mariamiller	maria,miller
gabrielgarciamarquez	gabriel,garcia,marquez
contraelsilenciomx	contra,el,silencio,mx
howoldareyou	how,old,are,you
poppedwheatthins	popped,wheat,thins
7millionandcounting	7,million,and,counting
bahraingp	bahrain,gp
bjpmanifesto	bjp,manifesto
happybirthdayshakespeare	happy,birthday,shakespeare

votekatniss	vote,kat,niss
mgwv	mg,wv
maduromasacraaltachira	maduro,ma,sacra,alta,chira
spinnrtaylorswift	spin,nr,taylor,swift
aappositive	a,appositive
heartbleed	heart,bleed
sfbatkid	sfb,at,kid
indvsban	in,dvs,ban
</pre>
<a href="tag_segments.txt">Segmentation for hashtags with atleast 1000 tweets</a>
<br>Reference http://norvig.com/ngrams/, Using code and data by Peter Norvig

<h4> 15 Sep </h4>

<h3>Effect of adding feature categories at different prediction thresholds</h3>
<img src="plots/featureEffect_1.png" width="1000" height="600" />

<h3>Results on different geographies (Table 8)</h3>
<pre>
	India 	
Set		AUC		AUC_prev	Prec		Prec_prev	Rec		Rec_prev	FM		FM_prev
E1		0.169		0.070		0.1759		0.113		0.5368		0.315		0.2649		0.165
E		0.233		0.327		0.2857		0.388		0.3824		0.308		0.327		0.344
E+N		0.274		0.319		0.2837		0.432		0.4338		0.257		0.343		0.322
E+C		0.274		0.327		0.3868		0.4		0.3015		0.324		0.3388		0.358
E+N+C		0.336		0.361		0.3134		0.49		0.4632		0.36		0.3739		0.415

	London 	
Set		AUC		AUC_prev	Prec		Prec_prev	Rec 		Rec_prev	FM		FM_prev
E1		0.087		0.091		0.0974		0.093		0.2909		0.364		0.1459		0.148
E		0.123		0.113		0.1422		0.141		0.1939		0.193		0.1641		0.163
E+N		0.121		0.115		0.0896		0.105		0.303		0.188		0.1383		0.135
E+C		0.179		0.189		0.1946		0.166		0.2182		0.321		0.2057		0.219
E+N+C		0.182		0.18		0.2297		0.17		0.2061		0.333		0.2173		0.225

	Quito 	
Set		AUC		AUC_prev	Prec		Prec_prev	Rec		Rec_prev	FM		FM_prev
E1		0.069		0.148		0.0764		0.187		0.4575		0.163		0.1309		0.174
E		0.082		0.168		0.0909	 	0.283		0.3529		0.15		0.1446		0.197
E+N		0.172		0.22		0.1818		0.338		0.2745		0.17		0.2188		0.226
E+C		0.1		0.172		0.1063		0.289		0.3987		0.157		0.1678		0.203
E+N+C		0.187		0.241		0.2135		0.449		0.268		0.203		0.2377		0.279
</pre>

<h3>Effect of size of training data</h3>
Computed by taking subsample (without replacement) of the training set in each fold of cross-validation
<pre>
Size	AUPRC
50.00%	0.258
60.00%	0.257
70.00%	0.258
80.00%	0.248
90.00%	0.26
100.00%	0.275
</pre>

<h3>Probability density plots (Figure 2)</h3>
<img src="plots/feature_distribution_plot/Average rate of change of conductance (last 50).png" width="800" height="400" />
<img src="plots/feature_distribution_plot/Growth Rate.png" width="800" height="400" />
<img src="plots/feature_distribution_plot/Number of Adopters with Heavy Following.png" width="800" height="400" />
<img src="plots/feature_distribution_plot/Number of Infected Geographies.png" width="800" height="400" />

<h3>Cumulative distribution of observation time of topics (Figure 1)</h3>
<img src="plots/cumulativePlotObsTime_modified.png" width="800" height="400" />
<br>
Median time: 
TwitDat 12.38 days,
WengDat 13.03 days

<h3>Urgency of prediction (Figure 3 and values)</h3>
<img src="plots/urgencyPrediction.png" width="800" height="400" />
<pre>
Threshold	AUC
250		0.1031
500		0.1234
1000		0.1816
1500		0.2747
2000		0.3090
2500		0.3324
3000		0.4169
</pre>
<h4> 08 Sep </h4>

<h3> Results for different feature combinations</h3>
<pre>
Feature		AUC		AUC_prev	Prec		Prec_prev	Rec		Rec_prev	FM		FM_prev
Random		0.063		0.035		0.063		0.035		0.50		0.50		0.1119		.0654
E1		0.110		0.176		0.1042		0.1316		0.4181		0.2655		0.1669		0.176
E		0.181		0.215		0.2103		0.1729		0.2994		0.2316		0.2471		0.198
E+N		0.192		0.233		0.2839		0.239 		0.2599		0.2768		0.2714		0.2565
E+G		0.215		0.214		0.2477		0.2114		0.3107		0.209 		0.2757		0.2102
E+C		0.258		0.269		0.2993		0.2806		0.4802		0.3107		0.3688		0.2949
E+N+G		0.186		0.221		0.2099		0.221 		0.3333		0.2372		0.2576		0.2288
E+G+C		0.266		0.26 		0.3609		0.2783		0.3446		0.3333		0.3526		0.3033
E+N+C		0.269		0.274		0.3598		0.2837		0.3333		0.3446		0.346		0.3112
E+N+G+C		0.275		0.261		0.3171		0.2809		0.3672		0.3333		0.3403		0.3049
</pre>
<h4> 06 Sep </h4>
<h3> Moving window Conductance plots for particular hashtags</h3>
<h3> Window size 25, Disjoint windows</h3>
<img src="plots/moving_conductance/tweet_wise/disjoint_25kselfie_t.png" width="800" height="400" />
<img src="plots/moving_conductance/tweet_wise/disjoint_100thingstodobeforeidie_t.png" width="800" height="400" />
<img src="plots/moving_conductance/tweet_wise/disjoint_7millionandcounting_t.png" width="800" height="400" />
<img src="plots/moving_conductance/tweet_wise/disjoint_cancelcolbert_t.png" width="800" height="400" />
<img src="plots/moving_conductance/tweet_wise/disjoint_heartbleed_t.png" width="800" height="400" />
<h3> Window size 100, Shift by 1</h3>
<img src="plots/moving_conductance/tweet_wise/smallwindow_25kselfie_t.png" width="800" height="400" />
<img src="plots/moving_conductance/tweet_wise/smallwindow_100thingstodobeforeidie_t.png" width="800" height="400" />
<img src="plots/moving_conductance/tweet_wise/smallwindow_7millionandcounting_t.png" width="800" height="400" />
<img src="plots/moving_conductance/tweet_wise/smallwindow_cancelcolbert_t.png" width="800" height="400" />
<img src="plots/moving_conductance/tweet_wise/smallwindow_heartbleed_t.png" width="800" height="400" />
<h3> Comparison with number of adopters (disjoint window)</h3>
<img src="plots/moving_conductance/time_wise/disjoint_100thingstodobeforeidie.png" width="800" height="400" />
<img src="plots/moving_conductance/time_wise/disjoint_25kselfie.png" width="800" height="400" />
<img src="plots/moving_conductance/time_wise/disjoint_cancelcolbert.png" width="800" height="400" />
<img src="plots/moving_conductance/time_wise/disjoint_heartbleed.png" width="800" height="400" />
<h4> 26 Aug </h4>

<h3> Feature plots for median values</h3>
Shaded region shows the upper(75%) and lower(25%) percentiles and line shows the median value<br/>
<h3> Evolution based features</h3>
<img src="plots/Number of Retweets_median.png" width="800" height="500" />
<img src="plots/Number of Adopters_median.png" width="800" height="500" />
<h3> Network based features</h3>
<img src="plots/Size of largest Connected Component_median.png" width="800" height="500" />
<img src="plots/Number of Adopters with Heavy Following_median.png" width="800" height="500" />
<h3> Geography based features</h3>
<img src="plots/Number of Infected Geographies_median.png" width="800" height="500" />
<img src="plots/Fraction of Intra Geography Activity (Retweets)_median.png" width="800" height="500" />
<h3> Conductance based features</h3>
<img src="plots/Second order derivative of conductance_median.png" width="800" height="500" />
<img src="plots/Average rate of change of conductance (last 100)_median.png" width="800" height="500" />
<img src="plots/Second order derivative of conductance_median_1.png" width="800" height="500" />
<img src="plots/Absolute value of the cumulative conductance_median.png" width="800" height="500" />
<h4> 24 Aug </h4>

<h3>Sample simulation run</h3>
alpha 0.05<br>
beta 6.0<br>
100thingstodobeforeidie 630 1397818252<br>
631 1397818271 631<br>
632 1397818282 633<br>
633 1397818286 634<br>
634 1397818318 635<br>
635 1397818318 637<br>
636 1397818338 637<br>
637 1397818343 638<br>
638 1397818352 639<br>
639 1397818363 639<br>
640 1397818366 641<br>
641 1397818372 642<br>
642 1397818393 642<br>
643 1397818399 643<br>
644 1397818410 643<br>
645 1397818417 643<br>
646 1397818421 643<br>
647 1397818425 643<br>
648 1397818427 644<br>
649 1397818436 644<br>
650 1397818437 644<br>
651 1397818437 645<br>
652 1397818439 646<br>
653 1397818458 647<br>
654 1397818476 649<br>
655 1397818486 649<br>
656 1397818507 650<br>
657 1397818514 652<br>
658 1397818516 653<br>
659 1397818562 655<br>
660 1397818562 657<br>
661 1397818564 658<br>
662 1397818568 659<br>
663 1397818570 659<br>
664 1397818570 660<br>
665 1397818579 661<br>
666 1397818582 663<br>
667 1397818591 664<br>
668 1397818598 666<br>
669 1397818602 666<br>
670 1397818605 666<br>
671 1397818635 667<br>
672 1397818647 667<br>
673 1397818654 667<br>
674 1397818661 667<br>
675 1397818679 668<br>
676 1397818696 669<br>
677 1397818726 671<br>
678 1397818747 671<br>
679 1397818761 672<br>
680 1397818786 672<br>
681 1397818808 673<br>
682 1397818838 674<br>
683 1397818856 674<br>
684 1397818879 675<br>
685 1397818894 676<br>
686 1397818918 676<br>
687 1397818932 676<br>
688 1397818947 679<br>
689 1397818955 681<br>
690 1397818957 681<br>
691 1397818963 681<br>
692 1397819001 682<br>
693 1397819026 683<br>
694 1397819052 684<br>
695 1397819122 684<br>
696 1397819138 688<br>
697 1397819155 690<br>
698 1397819169 691<br>
699 1397819171 692<br>
700 1397819173 694<br>
701 1397819175 694<br>
702 1397819176 695<br>
703 1397819186 700<br>
704 1397819190 705<br>
705 1397819209 705<br>
706 1397819212 705<br>
707 1397819217 707<br>
708 1397819225 708<br>
709 1397819240 711<br>
710 1397819246 716<br>
711 1397819251 716<br>
712 1397819255 720<br>
713 1397819259 724<br>
714 1397819264 726<br>
715 1397819269 729<br>
716 1397819302 732<br>
717 1397819326 735<br>
718 1397819332 740<br>
719 1397819353 745<br>
720 1397819364 747<br>
721 1397819388 754<br>

<h4> 26 Aug </h4>

<h3> Change in feature values with number of tweets of the hashtag </h3>
Figures are for top 2 features in each category. Feature values are averaged over all the topics.<br/>
Figures on the left are for non-viral hashtags and those on the right are for viral hashtags.<br/>
<h3> Evolution based features</h3>
<img src="plots/Number of Retweets.png" width="800" height="500" />
<img src="plots/Number of Adopters.png" width="800" height="500" />
<h3> Network based features</h3>
<img src="plots/Size of largest Connected Component.png" width="800" height="500" />
<img src="plots/Number of Adopters with Heavy Following.png" width="800" height="500" />
<h3> Geography based features</h3>
<img src="plots/Number of Infected Geographies.png" width="800" height="500" />
<img src="plots/Fraction of Intra Geography Activity (Retweets).png" width="800" height="500" />
<h3> Conductance based features</h3>
<img src="plots/Second order derivative of conductance.png" width="800" height="500" />
<img src="plots/Average rate of change of conductance (last 100).png" width="800" height="500" />
<h4> 19 Aug </h4>

<h3> Change in feature values with number of tweets of the hashtag </h3>
Figures are for top 2 features in each category. Hashtags were selected from those which were correctly predicted with high prediction probabilites (returned by trained model).<br/>
Figures on the left are for non-viral hashtags and those on the right are for viral hashtags.<br/>
Viral #tags: 7millionandcounting, heartbleed<br/>
Non-viral #tags: 100thingstodobeforeidie, followmee3<br/>
<h3> Evolution based features</h3>
<img src="plots/timeline_Number of Retweets.jpeg" width="900" height="800" />
<img src="plots/timeline_Number of Adopters.jpeg" width="900" height="800" />
<h3> Network based features</h3>
<img src="plots/timeline_Size of largest Connected Component.jpeg" width="900" height="800" />
<img src="plots/timeline_Number of Adopters with Heavy Following.jpeg" width="900" height="800" />
<h3> Geography based features</h3>
<img src="plots/timeline_Number of Infected Geographies.jpeg" width="900" height="800" />
<img src="plots/timeline_Fraction of Intra Geography Activity (Retweets).jpeg" width="900" height="800" />
<h3> Conductance based features</h3>
<img src="plots/timeline_Second order derivative of conductance.jpeg" width="900" height="800" />
<img src="plots/timeline_Average rate of change of conductance (last 100).jpeg" width="900" height="800" />
<h4> 11 Aug </h4>
</body>
</html>
